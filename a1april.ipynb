{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9eaaf6c-c73d-422f-92ee-1cfdb895d3ef",
   "metadata": {},
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45e533-b2da-4ea9-966f-6ee5cd3ca223",
   "metadata": {},
   "source": [
    "Linear regression and logistic regression models are both supervised learning algorithms, but they are used for different types of problems.\n",
    "\n",
    "Linear regression is used for predicting continuous numeric values. It establishes a linear relationship between the independent variables (features) and the dependent variable (target) by fitting a line to the data points. The goal is to minimize the difference between the predicted values and the actual values, typically using a cost function like mean squared error. For example, linear regression can be used to predict house prices based on features like square footage, number of bedrooms, and location.\n",
    "\n",
    "Logistic regression, on the other hand, is used for binary classification problems where the target variable has two possible outcomes (e.g., true/false, yes/no). It estimates the probability of the target belonging to a certain class based on the input features. Logistic regression uses a logistic (sigmoid) function to map the linear combination of the features to a probability value between 0 and 1. If the probability is above a certain threshold (e.g., 0.5), the instance is classified as one class; otherwise, it is classified as the other class. For example, logistic regression can be used to predict whether a customer will churn or not based on their demographic and behavioral attributes.\n",
    "\n",
    "In scenarios where the target variable is binary and the goal is to classify instances into one of the two classes, logistic regression is more appropriate than linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c16c8-1a39-4e90-a18d-984e062a4b56",
   "metadata": {},
   "source": [
    "# Quetion : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a92de7-25ac-4e85-8efb-15d869cbcfad",
   "metadata": {},
   "source": [
    "The cost function used in logistic regression is called the logistic loss function or the binary cross-entropy loss. It measures the difference between the predicted probabilities and the true labels of the training instances.\n",
    "\n",
    "The logistic loss function is defined as:\n",
    "\n",
    "Cost = - (1/m) * Σ [y * log(y_hat) + (1-y) * log(1-y_hat)]\n",
    "\n",
    "where:\n",
    "\n",
    "m is the number of training instances\n",
    "y is the true label (0 or 1) for an instance\n",
    "y_hat is the predicted probability for the instance\n",
    "The goal is to minimize the cost function by finding the optimal values for the model's parameters, typically using optimization algorithms like gradient descent or its variations. Gradient descent iteratively adjusts the parameters in the direction of steepest descent of the cost function to reach the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c431d-f883-497f-9a80-c50bc0b2ee08",
   "metadata": {},
   "source": [
    "# Quetion : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46965f21-30ba-4588-b5ed-5fc88306561b",
   "metadata": {},
   "source": [
    "Regularization in logistic regression is a technique used to prevent overfitting, which occurs when the model fits the training data too closely but fails to generalize well to unseen data. Regularization adds a penalty term to the cost function that discourages complex models with large parameter values.\n",
    "\n",
    "The two commonly used regularization techniques in logistic regression are L1 regularization (Lasso) and L2 regularization (Ridge).\n",
    "\n",
    "L1 regularization adds the sum of the absolute values of the parameters to the cost function. It tends to push the coefficients of irrelevant or less important features to zero, effectively performing feature selection.\n",
    "\n",
    "L2 regularization adds the sum of the squared values of the parameters to the cost function. It encourages smaller parameter values but does not force them to zero. L2 regularization can help in reducing the impact of multicollinearity (high correlation) among the independent variables.\n",
    "\n",
    "The amount of regularization is controlled by a hyperparameter called the regularization parameter (λ or alpha). By tuning this parameter, the balance between fitting the training data and preventing overfitting can be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07253a4-9bb3-40df-b62d-bd4759c31789",
   "metadata": {},
   "source": [
    "# Quetion : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ee54e-c70c-4a77-ad06-06aa21426e45",
   "metadata": {},
   "source": [
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classification model, such as logistic regression. It illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) for different classification thresholds.\n",
    "\n",
    "To create an ROC curve for a logistic regression model, the following steps are typically followed:\n",
    "\n",
    "Train the logistic regression model on the training data.\n",
    "Obtain the predicted probabilities for the instances in the validation or test set.\n",
    "Sort the instances based on the predicted probabilities.\n",
    "Starting from the lowest probability threshold, classify the instances accordingly, considering each threshold as the cutoff for classifying an instance as positive or negative.\n",
    "Calculate the true positive rate (TPR) and false positive rate (FPR) for each threshold.\n",
    "Plot the TPR on the y-axis against the FPR on the x-axis.\n",
    "The resulting curve represents the ROC curve.\n",
    "The ROC curve helps evaluate the performance of the logistic regression model by providing insights into its discriminatory power and the ability to balance true positives and false positives. The area under the ROC curve (AUC-ROC) is often used as a summary metric for model performance, with higher values indicating better performance. A model with an AUC-ROC of 0.5 performs no better than random guessing, while a model with an AUC-ROC of 1.0 represents a perfect classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220cb53-b305-4525-ae40-552dcc52037e",
   "metadata": {},
   "source": [
    "# Quetion : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf12f57-afaa-4ac9-ac8f-1385480844b7",
   "metadata": {},
   "source": [
    "Feature selection techniques in logistic regression aim to identify and select the most relevant and informative features for predicting the target variable. Some common techniques include:\n",
    "\n",
    "a. Univariate Selection: This method involves selecting features based on their individual statistical significance. Features are evaluated independently using statistical tests like chi-square test, ANOVA, or correlation with the target variable. The top-k features with the highest scores are selected.\n",
    "\n",
    "b. Recursive Feature Elimination (RFE): RFE is an iterative technique that starts with all features and eliminates the least important ones at each step. The model is trained on the remaining features, and their importance is assessed. This process continues until a desired number of features is reached.\n",
    "\n",
    "c. Regularization: As mentioned earlier, L1 regularization (Lasso) can perform feature selection by shrinking the coefficients of less important features to zero. Features with non-zero coefficients are selected.\n",
    "\n",
    "d. Principal Component Analysis (PCA): PCA transforms the original features into a new set of uncorrelated variables called principal components. The principal components are ranked based on their ability to explain the variance in the data, and a subset of the top components is chosen as features.\n",
    "\n",
    "These techniques help improve the model's performance by reducing overfitting, reducing the dimensionality of the feature space, and selecting the most relevant features, thereby improving interpretability and reducing computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2a2ca-86e4-4f34-9fa1-aa2155e71807",
   "metadata": {},
   "source": [
    "# Quetion : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad28bb-32f3-459b-84a0-a7b047b55caf",
   "metadata": {},
   "outputs": [],
   "source": [
    ". Handling imbalanced datasets in logistic regression is important when the classes in the target variable are not represented equally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ba024-06cf-4b86-89a8-568330b14092",
   "metadata": {},
   "source": [
    "# Quetion : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c31b8-2ea1-4596-81a5-93990007bd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
